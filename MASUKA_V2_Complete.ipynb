{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® MASUKA V2 - Complete Training & Generation Platform\n",
    "\n",
    "**Train Flux LoRAs and Generate Images with GPU in Google Colab**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Quick Start Guide\n",
    "\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí **T4 GPU** (or A100 for faster)\n",
    "2. **Run Cell 1** ‚ñ∂Ô∏è - Complete setup (~10 minutes)\n",
    "3. **Run Cell 2** ‚ñ∂Ô∏è - Upload images & train (~30-60 minutes)\n",
    "4. **Run Cell 3** ‚ñ∂Ô∏è - Generate images with your trained LoRA!\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What You'll Get\n",
    "\n",
    "- ‚úÖ **Training**: Train custom Flux LoRAs with 15-25 images\n",
    "- ‚úÖ **Generation**: Generate images using trained LoRAs\n",
    "- ‚úÖ **Storage**: Models saved to S3 automatically\n",
    "- ‚úÖ **API**: Full REST API with documentation\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Important Notes:**\n",
    "- Keep this notebook running during training\n",
    "- Training: 45min (T4) or 20min (A100)\n",
    "- Generation: ~1 min per image\n",
    "- All models saved to S3 bucket\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üöÄ CELL 1: Complete Setup (Run Once)\n",
    "#@markdown This will take ~10 minutes. Watch for your API URL at the end!\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import subprocess\n",
    "from IPython.display import clear_output, HTML\n",
    "\n",
    "def print_step(emoji, title, status=\"\"):\n",
    "    print(f\"\\n{emoji} {title}\")\n",
    "    print(\"=\"*60)\n",
    "    if status:\n",
    "        print(status)\n",
    "\n",
    "def run_command(cmd, description, silent=True):\n",
    "    \"\"\"Run a command and handle output\"\"\"\n",
    "    print(f\"  {description}...\", end=\"\", flush=True)\n",
    "    try:\n",
    "        if silent:\n",
    "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n",
    "        else:\n",
    "            result = subprocess.run(cmd, shell=True, check=True)\n",
    "        print(\" ‚úÖ\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\" ‚ùå\\n{e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*70)\n",
    "print(\"     üé® MASUKA V2 - Complete Setup Starting\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Check GPU\n",
    "# ============================================================\n",
    "print_step(\"1Ô∏è‚É£\", \"Checking GPU\")\n",
    "run_command(\"nvidia-smi -L\", \"Detecting GPU\", silent=True)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"  GPU: {gpu_name}\")\n",
    "    print(f\"  VRAM: {vram:.1f} GB\")\n",
    "    print(\"  ‚úÖ GPU Ready!\")\n",
    "else:\n",
    "    print(\"  ‚ùå No GPU detected!\")\n",
    "    print(\"  Go to: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Clone Repository\n",
    "# ============================================================\n",
    "print_step(\"2Ô∏è‚É£\", \"Cloning MASUKA V2 Repository\")\n",
    "if not os.path.exists('/content/masuka-v2'):\n",
    "    run_command(\n",
    "        \"git clone https://github.com/SamuelD27/masuka.git /content/masuka-v2\",\n",
    "        \"Cloning from GitHub\",\n",
    "        silent=False\n",
    "    )\n",
    "else:\n",
    "    print(\"  ‚úÖ Repository exists\")\n",
    "    os.chdir('/content/masuka-v2')\n",
    "    run_command(\"git pull origin main\", \"Updating repository\", silent=False)\n",
    "\n",
    "os.chdir('/content/masuka-v2')\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Install Dependencies\n",
    "# ============================================================\n",
    "print_step(\"3Ô∏è‚É£\", \"Installing Python Dependencies (Phase 3 includes ML libs)\")\n",
    "print(\"  This may take 3-5 minutes...\")\n",
    "\n",
    "# Install from requirements.txt\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"backend/requirements.txt\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "# Install pyngrok for tunnel\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyngrok\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "print(\"  ‚úÖ All dependencies installed\")\n",
    "\n",
    "# Verify key packages\n",
    "import fastapi, celery, pydantic, diffusers\n",
    "print(f\"  FastAPI: {fastapi.__version__}\")\n",
    "print(f\"  Celery: {celery.__version__}\")\n",
    "print(f\"  Pydantic: {pydantic.__version__}\")\n",
    "print(f\"  Diffusers: {diffusers.__version__}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Install SimpleTuner\n",
    "# ============================================================\n",
    "print_step(\"4Ô∏è‚É£\", \"Installing SimpleTuner\")\n",
    "if not os.path.exists('/content/SimpleTuner'):\n",
    "    run_command(\n",
    "        \"git clone -q https://github.com/bghira/SimpleTuner /content/SimpleTuner\",\n",
    "        \"Cloning SimpleTuner\",\n",
    "        silent=False\n",
    "    )\n",
    "    \n",
    "    print(\"  Installing SimpleTuner package...\", end=\"\", flush=True)\n",
    "    os.chdir('/content/SimpleTuner')\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    print(\" ‚úÖ\")\n",
    "    \n",
    "    os.chdir('/content/masuka-v2')\n",
    "    print(\"  ‚úÖ SimpleTuner ready at /content/SimpleTuner\")\n",
    "else:\n",
    "    print(\"  ‚úÖ SimpleTuner already installed\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: Setup PostgreSQL\n",
    "# ============================================================\n",
    "print_step(\"5Ô∏è‚É£\", \"Setting up PostgreSQL\")\n",
    "run_command(\"apt-get update -qq && apt-get install -y -qq postgresql postgresql-contrib\", \n",
    "            \"Installing PostgreSQL\")\n",
    "run_command(\"service postgresql start\", \"Starting PostgreSQL\")\n",
    "time.sleep(2)\n",
    "\n",
    "commands = [\n",
    "    \"DROP DATABASE IF EXISTS masuka;\",\n",
    "    \"DROP USER IF EXISTS masuka;\",\n",
    "    \"CREATE USER masuka WITH PASSWORD 'password123';\",\n",
    "    \"CREATE DATABASE masuka OWNER masuka;\",\n",
    "    \"GRANT ALL PRIVILEGES ON DATABASE masuka TO masuka;\"\n",
    "]\n",
    "for cmd in commands:\n",
    "    subprocess.run([\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", cmd], \n",
    "                   capture_output=True)\n",
    "print(\"  ‚úÖ Database 'masuka' created\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: Setup Redis\n",
    "# ============================================================\n",
    "print_step(\"6Ô∏è‚É£\", \"Setting up Redis\")\n",
    "run_command(\"apt-get install -y -qq redis-server\", \"Installing Redis\")\n",
    "run_command(\"redis-server --daemonize yes\", \"Starting Redis\")\n",
    "time.sleep(1)\n",
    "run_command(\"redis-cli ping\", \"Testing Redis\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 7: Configure Environment\n",
    "# ============================================================\n",
    "print_step(\"7Ô∏è‚É£\", \"Configuring Environment\")\n",
    "\n",
    "env_content = \"\"\"# MASUKA V2 Colab Environment\n",
    "APP_NAME=MASUKA V2\n",
    "DEBUG=true\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=postgresql://masuka:password123@localhost:5432/masuka\n",
    "\n",
    "# Redis\n",
    "REDIS_URL=redis://localhost:6379/0\n",
    "CELERY_BROKER_URL=redis://localhost:6379/0\n",
    "CELERY_RESULT_BACKEND=redis://localhost:6379/0\n",
    "\n",
    "# JWT\n",
    "SECRET_KEY=colab-secret-key-change-in-production\n",
    "ALGORITHM=HS256\n",
    "ACCESS_TOKEN_EXPIRE_MINUTES=1440\n",
    "\n",
    "# Storage (AWS S3)\n",
    "S3_BUCKET=masuka-v2\n",
    "AWS_ACCESS_KEY_ID=AKIAWMUZGDEJYY6UYN5A\n",
    "AWS_SECRET_ACCESS_KEY=AZxb199Vcy/aI5CGyvefWy1MhLGq1x4tKcCmq0NG\n",
    "S3_REGION=ap-southeast-1\n",
    "\n",
    "# Hugging Face (for Flux model download)\n",
    "HF_TOKEN=hf_your_token_here\n",
    "\n",
    "# Paths\n",
    "SIMPLETUNER_PATH=/content/SimpleTuner\n",
    "MODELS_PATH=/content/models\n",
    "TEMP_PATH=/tmp/masuka\n",
    "\n",
    "# CORS\n",
    "CORS_ORIGINS=*\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs('/content/masuka-v2/backend', exist_ok=True)\n",
    "with open('/content/masuka-v2/backend/.env', 'w') as f:\n",
    "    f.write(env_content)\n",
    "print(\"  ‚úÖ Environment configured\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 8: Initialize Database & Directories\n",
    "# ============================================================\n",
    "print_step(\"8Ô∏è‚É£\", \"Initializing Database & Directories\")\n",
    "os.makedirs('/tmp/masuka/uploads', exist_ok=True)\n",
    "os.makedirs('/tmp/masuka/training', exist_ok=True)\n",
    "os.makedirs('/tmp/masuka/generated', exist_ok=True)\n",
    "os.makedirs('/tmp/masuka/model_cache', exist_ok=True)\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "print(\"  ‚úÖ Directories created\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 9: Start FastAPI Backend\n",
    "# ============================================================\n",
    "print_step(\"9Ô∏è‚É£\", \"Starting FastAPI Backend\")\n",
    "os.chdir('/content/masuka-v2/backend')\n",
    "\n",
    "subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True)\n",
    "time.sleep(2)\n",
    "\n",
    "backend_process = subprocess.Popen(\n",
    "    ['uvicorn', 'app.main:app', '--host', '0.0.0.0', '--port', '8000', '--log-level', 'info'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"  ‚è≥ Waiting for backend to start...\")\n",
    "time.sleep(15)\n",
    "\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get('http://localhost:8000/health', timeout=5)\n",
    "    health = response.json()\n",
    "    print(f\"  ‚úÖ Backend running: {health['app']} v{health['version']}\")\n",
    "    \n",
    "    # Verify new Phase 3 endpoints\n",
    "    response = requests.get('http://localhost:8000/api/models/', timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"  ‚úÖ Models API loaded (Phase 3)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è  Backend may not be ready: {e}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 10: Start Celery Worker\n",
    "# ============================================================\n",
    "print_step(\"üîü\", \"Starting Celery Worker\")\n",
    "\n",
    "subprocess.run(['pkill', '-f', 'celery'], capture_output=True)\n",
    "time.sleep(2)\n",
    "\n",
    "celery_process = subprocess.Popen(\n",
    "    ['celery', '-A', 'app.tasks.celery_app', 'worker', \n",
    "     '--loglevel=info', '--concurrency=1', '-Q', 'training,generation'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"  ‚è≥ Waiting for Celery to start...\")\n",
    "time.sleep(5)\n",
    "print(f\"  ‚úÖ Celery worker running with training+generation queues (GPU: {gpu_name})\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 11: Expose via ngrok\n",
    "# ============================================================\n",
    "print_step(\"1Ô∏è‚É£1Ô∏è‚É£\", \"Creating Public URL\")\n",
    "\n",
    "from pyngrok import ngrok\n",
    "\n",
    "ngrok.set_auth_token(\"33u4PSfJRAAdkBVl0lmMTo7LebK_815Q5PcJK6h68hM5PUAyM\")\n",
    "ngrok.kill()\n",
    "time.sleep(2)\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"  ‚úÖ Tunnel created\")\n",
    "\n",
    "# ============================================================\n",
    "# FINAL OUTPUT\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ MASUKA V2 Setup Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüì° Your Public API URL:\")\n",
    "print(f\"   {public_url}\")\n",
    "print(f\"\\nüìö Interactive Docs:\")\n",
    "print(f\"   {public_url}/docs\")\n",
    "print(f\"\\nüîç Health Check:\")\n",
    "print(f\"   {public_url}/health\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚úÖ Services Running:\")\n",
    "print(\"   ‚Ä¢ PostgreSQL Database\")\n",
    "print(\"   ‚Ä¢ Redis Cache & Queue\")\n",
    "print(\"   ‚Ä¢ FastAPI Backend\")\n",
    "print(f\"   ‚Ä¢ Celery Worker (GPU: {gpu_name})\")\n",
    "print(\"   ‚Ä¢ SimpleTuner Ready\")\n",
    "print(\"   ‚Ä¢ Flux Generator Ready (Phase 3)\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this notebook running during training & generation!\")\n",
    "print(\"\\nüìñ Next Steps:\")\n",
    "print(\"   ‚Üí Run Cell 2 to train a LoRA\")\n",
    "print(\"   ‚Üí Run Cell 3 to generate images\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store for other cells\n",
    "API_URL = str(public_url).replace('NgrokTunnel: \"', '').split('\"')[0]\n",
    "if not API_URL.startswith('http'):\n",
    "    API_URL = str(public_url)\n",
    "print(f\"\\n‚úÖ API_URL stored: {API_URL}\")\n",
    "\n",
    "BACKEND_PROCESS = backend_process\n",
    "CELERY_PROCESS = celery_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üì§ CELL 2: Train LoRA - Upload Images & Start Training\n",
    "#@markdown Upload 15-25 images of your subject (JPG/PNG). Training starts automatically and monitors progress.\n",
    "\n",
    "from google.colab import files\n",
    "import requests\n",
    "import io\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ensure API_URL is set\n",
    "if not isinstance(API_URL, str):\n",
    "    API_URL = str(API_URL).replace('NgrokTunnel: \"', '').split('\"')[0]\n",
    "print(f\"Using API: {API_URL}\")\n",
    "\n",
    "# Get parameters\n",
    "dataset_name = \"My LoRA Dataset\" #@param {type:\"string\"}\n",
    "trigger_word = \"TOK\" #@param {type:\"string\"}\n",
    "learning_rate = 0.0001 #@param {type:\"number\"}\n",
    "training_steps = 2000 #@param {type:\"slider\", min:500, max:5000, step:100}\n",
    "\n",
    "print(\"üì§ Upload Your Training Images\")\n",
    "print(\"=\"*60)\n",
    "print(\"Click 'Choose Files' and select 15-25 images...\\n\")\n",
    "\n",
    "# Upload files\n",
    "uploaded = files.upload()\n",
    "print(f\"\\n‚úÖ Received {len(uploaded)} files\")\n",
    "\n",
    "# Prepare files\n",
    "file_list = []\n",
    "for filename, data in uploaded.items():\n",
    "    if filename.lower().endswith(('.jpg', '.jpeg')):\n",
    "        content_type = 'image/jpeg'\n",
    "    elif filename.lower().endswith('.png'):\n",
    "        content_type = 'image/png'\n",
    "    elif filename.lower().endswith('.webp'):\n",
    "        content_type = 'image/webp'\n",
    "    else:\n",
    "        continue\n",
    "    file_list.append(('files', (filename, io.BytesIO(data), content_type)))\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nüì¶ Creating dataset...\")\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/api/datasets/\",\n",
    "    json={\"name\": dataset_name, \"trigger_word\": trigger_word}\n",
    ")\n",
    "\n",
    "if response.status_code != 201:\n",
    "    print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n",
    "    raise Exception(f\"Failed to create dataset\")\n",
    "\n",
    "dataset = response.json()\n",
    "dataset_id = dataset['id']\n",
    "print(f\"‚úÖ Dataset: {dataset_id}\")\n",
    "\n",
    "# Upload images\n",
    "print(\"\\nüì§ Uploading to server...\")\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/api/datasets/{dataset_id}/upload\",\n",
    "    files=file_list\n",
    ")\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n",
    "    raise Exception(f\"Failed to upload files\")\n",
    "\n",
    "result = response.json()\n",
    "print(f\"‚úÖ Uploaded {result['uploaded_count']} images\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/api/training/flux\",\n",
    "    json={\n",
    "        \"name\": f\"{dataset_name} - v1\",\n",
    "        \"model_type\": \"flux_image\",\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"steps\": training_steps,\n",
    "        \"network_dim\": 32,\n",
    "        \"network_alpha\": 16,\n",
    "        \"resolution\": 1024,\n",
    "        \"trigger_word\": trigger_word\n",
    "    }\n",
    ")\n",
    "\n",
    "if response.status_code not in [200, 201]:\n",
    "    print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n",
    "    raise Exception(f\"Failed to start training\")\n",
    "\n",
    "training = response.json()\n",
    "SESSION_ID = training['session_id']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training Started!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "print(f\"Status: {training['status']}\")\n",
    "print(f\"Task ID: {training['task_id']}\")\n",
    "print(f\"\\n‚è±Ô∏è  Estimated Time: 45-60 minutes (T4) or 20-30 minutes (A100)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Monitor progress\n",
    "print(\"\\n\\nüìä Monitoring Training Progress...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_step = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(f\"{API_URL}/api/training/{SESSION_ID}\")\n",
    "        status = response.json()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(f\"üé® {status['name']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        current_status = status['status']\n",
    "        status_emoji = {\n",
    "            'pending': '‚è≥', 'training': 'üîÑ',\n",
    "            'completed': '‚úÖ', 'failed': '‚ùå', 'cancelled': 'üõë'\n",
    "        }\n",
    "        print(f\"\\nStatus: {status_emoji.get(current_status, '‚ùì')} {current_status.upper()}\")\n",
    "        \n",
    "        if status.get('current_step') and status.get('total_steps'):\n",
    "            current = status['current_step']\n",
    "            total = status['total_steps']\n",
    "            progress = (current / total) * 100\n",
    "            \n",
    "            print(f\"\\nProgress: {progress:.1f}% ({current:,}/{total:,} steps)\")\n",
    "            \n",
    "            bar_length = 50\n",
    "            filled = int(bar_length * progress / 100)\n",
    "            bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)\n",
    "            print(f\"[{bar}] {progress:.1f}%\")\n",
    "            \n",
    "            if status.get('current_loss'):\n",
    "                print(f\"\\nLoss: {status['current_loss']:.6f}\")\n",
    "            \n",
    "            if current > last_step and last_step > 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                time_per_step = elapsed / current\n",
    "                eta_seconds = time_per_step * (total - current)\n",
    "                print(f\"\\nElapsed: {elapsed/60:.1f}m | ETA: {eta_seconds/60:.1f}m\")\n",
    "            \n",
    "            last_step = current\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        if current_status in ['completed', 'failed', 'cancelled']:\n",
    "            if current_status == 'completed':\n",
    "                print(\"\\nüéâ Training Completed!\")\n",
    "                print(\"‚úÖ Model uploaded to S3\")\n",
    "                print(f\"\\nüì¶ Session ID: {SESSION_ID}\")\n",
    "                print(\"\\nüí° Next: Run Cell 3 to generate images with this LoRA!\")\n",
    "            elif current_status == 'failed':\n",
    "                print(f\"\\n‚ùå Training Failed: {status.get('error_message', 'Unknown')}\")\n",
    "            else:\n",
    "                print(\"\\nüõë Training Cancelled\")\n",
    "            break\n",
    "        \n",
    "        print(\"\\n‚è≥ Updating in 10 seconds...\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Monitoring stopped. Training continues in background.\")\n",
    "        print(f\"üìã Session ID: {SESSION_ID}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üé® CELL 3: Generate Images with Trained LoRA\n",
    "#@markdown Generate images using your trained model. First time takes 10-15 min (Flux model download), then ~1 min per image.\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from IPython.display import clear_output, display, Image as IPImage\n",
    "import io\n",
    "\n",
    "# Ensure API_URL is set\n",
    "if not isinstance(API_URL, str):\n",
    "    API_URL = str(API_URL).replace('NgrokTunnel: \"', '').split('\"')[0]\n",
    "print(f\"Using API: {API_URL}\\n\")\n",
    "\n",
    "# Generation parameters\n",
    "prompt = \"photo of TOK person standing in a field, professional photography, golden hour lighting, 8k, highly detailed\" #@param {type:\"string\"}\n",
    "negative_prompt = \"blurry, low quality, distorted, ugly, deformed\" #@param {type:\"string\"}\n",
    "use_lora = True #@param {type:\"boolean\"}\n",
    "lora_weight = 0.8 #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
    "num_images = 2 #@param {type:\"slider\", min:1, max:4, step:1}\n",
    "num_inference_steps = 30 #@param {type:\"slider\", min:10, max:100, step:5}\n",
    "guidance_scale = 3.5 #@param {type:\"slider\", min:1.0, max:20.0, step:0.5}\n",
    "width = 1024 #@param {type:\"slider\", min:512, max:2048, step:64}\n",
    "height = 1024 #@param {type:\"slider\", min:512, max:2048, step:64}\n",
    "seed = None #@param {type:\"integer\"}\n",
    "\n",
    "# ============================================================\n",
    "# Step 1: Get available models\n",
    "# ============================================================\n",
    "print(\"üìã Fetching available models...\\n\")\n",
    "response = requests.get(f\"{API_URL}/api/models/\")\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"‚ùå Error fetching models: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "    raise Exception(\"Failed to fetch models\")\n",
    "\n",
    "models = response.json()\n",
    "\n",
    "if not models:\n",
    "    print(\"‚ö†Ô∏è  No trained models found!\")\n",
    "    print(\"\\nüí° Please run Cell 2 first to train a LoRA.\")\n",
    "    raise Exception(\"No models available\")\n",
    "\n",
    "print(f\"‚úÖ Found {len(models)} model(s):\\n\")\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"{i+1}. {model['name']} (v{model['version']})\")\n",
    "    print(f\"   ID: {model['id']}\")\n",
    "    print(f\"   Type: {model['model_type']}\")\n",
    "    print(f\"   Trigger: {model.get('trigger_word', 'N/A')}\")\n",
    "    print(f\"   Size: {model.get('file_size_mb', 'N/A')} MB\")\n",
    "    print()\n",
    "\n",
    "# Use the most recent model\n",
    "selected_model = models[0]\n",
    "model_id = selected_model['id'] if use_lora else None\n",
    "\n",
    "if use_lora:\n",
    "    print(f\"üéØ Using model: {selected_model['name']}\\n\")\n",
    "else:\n",
    "    print(f\"üéØ Using base Flux (no LoRA)\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 2: Start generation\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"üé® Starting Image Generation\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìù Prompt: {prompt}\")\n",
    "print(f\"üö´ Negative: {negative_prompt}\")\n",
    "print(f\"\\n‚öôÔ∏è  Settings:\")\n",
    "print(f\"   Images: {num_images}\")\n",
    "print(f\"   Steps: {num_inference_steps}\")\n",
    "print(f\"   Guidance: {guidance_scale}\")\n",
    "print(f\"   Size: {width}x{height}\")\n",
    "if use_lora:\n",
    "    print(f\"   LoRA Weight: {lora_weight}\")\n",
    "print(\"\\nüöÄ Submitting generation job...\")\n",
    "\n",
    "generation_request = {\n",
    "    \"prompt\": prompt,\n",
    "    \"negative_prompt\": negative_prompt,\n",
    "    \"num_images\": num_images,\n",
    "    \"num_inference_steps\": num_inference_steps,\n",
    "    \"guidance_scale\": guidance_scale,\n",
    "    \"width\": width,\n",
    "    \"height\": height\n",
    "}\n",
    "\n",
    "if use_lora and model_id:\n",
    "    generation_request[\"model_id\"] = model_id\n",
    "    generation_request[\"lora_weight\"] = lora_weight\n",
    "\n",
    "if seed is not None:\n",
    "    generation_request[\"seed\"] = seed\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/api/generate/image\",\n",
    "    json=generation_request\n",
    ")\n",
    "\n",
    "if response.status_code != 201:\n",
    "    print(f\"\\n‚ùå Error starting generation: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "    raise Exception(\"Failed to start generation\")\n",
    "\n",
    "generation = response.json()\n",
    "job_id = generation['job_id']\n",
    "\n",
    "print(f\"\\n‚úÖ Generation job created!\")\n",
    "print(f\"üìã Job ID: {job_id}\")\n",
    "print(f\"üîÑ Task ID: {generation['task_id']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚è≥ Generating images...\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° First generation takes 10-15 min (Flux model download)\")\n",
    "print(\"   Subsequent generations: ~1 min per image\")\n",
    "print(\"\\nüîÑ Polling for completion...\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 3: Monitor generation progress\n",
    "# ============================================================\n",
    "start_time = time.time()\n",
    "check_count = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(f\"{API_URL}/api/generate/{job_id}\")\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error checking status: {response.status_code}\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        \n",
    "        job_status = response.json()\n",
    "        status = job_status['status']\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        check_count += 1\n",
    "        \n",
    "        print(f\"[{elapsed:.0f}s] Check #{check_count}: {status}...\", end=\"\\r\")\n",
    "        \n",
    "        if status == 'completed':\n",
    "            clear_output(wait=True)\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üéâ Generation Complete!\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\n‚è±Ô∏è  Total time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "            \n",
    "            output_paths = job_status.get('output_paths', [])\n",
    "            print(f\"\\n‚úÖ Generated {len(output_paths)} image(s):\\n\")\n",
    "            \n",
    "            # Display images\n",
    "            for i, url in enumerate(output_paths):\n",
    "                print(f\"Image {i+1}:\")\n",
    "                print(f\"  URL: {url[:80]}...\")\n",
    "                \n",
    "                # Download and display\n",
    "                try:\n",
    "                    img_response = requests.get(url)\n",
    "                    if img_response.status_code == 200:\n",
    "                        display(IPImage(data=img_response.content))\n",
    "                        print()\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Could not display image: {e}\\n\")\n",
    "            \n",
    "            print(\"=\"*70)\n",
    "            print(\"\\nüí° Images saved to S3 bucket: masuka-v2/generated/\")\n",
    "            print(f\"\\nüìã Job ID: {job_id}\")\n",
    "            print(\"\\n‚ú® Generate more images by running this cell again!\")\n",
    "            print(\"=\"*70)\n",
    "            break\n",
    "            \n",
    "        elif status == 'failed':\n",
    "            print(f\"\\n\\n‚ùå Generation Failed!\")\n",
    "            error = job_status.get('error_message', 'Unknown error')\n",
    "            print(f\"Error: {error}\")\n",
    "            break\n",
    "            \n",
    "        elif status == 'processing':\n",
    "            # Still processing, continue polling\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            # Unknown status\n",
    "            time.sleep(5)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Monitoring stopped. Generation continues in background.\")\n",
    "        print(f\"üìã Job ID: {job_id}\")\n",
    "        print(f\"üí° Check status: {API_URL}/api/generate/{job_id}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during monitoring: {e}\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ You're All Set!\n",
    "\n",
    "### What You Can Do Now:\n",
    "\n",
    "1. **Train More LoRAs** - Run Cell 2 with different images\n",
    "2. **Generate More Images** - Run Cell 3 with different prompts\n",
    "3. **Experiment** - Try different parameters (steps, guidance, LoRA weight)\n",
    "\n",
    "### Your Models Are Saved:\n",
    "\n",
    "- **Location:** S3 Bucket `masuka-v2`\n",
    "- **Models:** `models/{session_id}/flux_lora.safetensors`\n",
    "- **Generations:** `generated/{job_id}/image_*.png`\n",
    "\n",
    "### API Endpoints:\n",
    "\n",
    "- **Health:** `{API_URL}/health`\n",
    "- **Docs:** `{API_URL}/docs`\n",
    "- **Models:** `{API_URL}/api/models/`\n",
    "- **Generate:** `{API_URL}/api/generate/image`\n",
    "- **Training:** `{API_URL}/api/training/`\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- **First generation is slow** - Flux model download (10GB)\n",
    "- **Use trigger word** - Include your trigger word in prompts (e.g., \"TOK\")\n",
    "- **Adjust LoRA weight** - 0.6-0.9 works best for most cases\n",
    "- **Higher steps = better quality** - But takes longer (try 30-50)\n",
    "\n",
    "---\n",
    "\n",
    "**Created by MASUKA V2** | Phase 3 Complete ‚úÖ\n",
    "\n",
    "**Questions?** Check the [documentation](https://github.com/SamuelD27/masuka)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
