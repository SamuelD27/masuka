{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ¨ MASUKA V2 - Complete Setup (Single Cell)\n",
    "\n",
    "**Train Flux LoRAs with GPU in Google Colab**\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Quick Start\n",
    "\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ T4 GPU\n",
    "2. **Run the cell below** â–¶ï¸\n",
    "3. **Wait ~10 minutes** for complete setup\n",
    "4. **Get your API URL** from the output\n",
    "5. **Use the helper cells below** to upload images and train!\n",
    "\n",
    "---\n",
    "\n",
    "**âš ï¸ Important:**\n",
    "- Keep this notebook running during training\n",
    "- Training takes ~45min (T4) or ~20min (A100)\n",
    "- Models saved to S3 automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "complete_setup",
    "cellView": "form"
   },
   "outputs": [],
   "source": "#@title ğŸš€ Run Complete MASUKA V2 Setup\n#@markdown This will take ~10 minutes. Watch the output for your API URL!\n\nimport os\nimport sys\nimport time\nimport subprocess\nfrom IPython.display import clear_output, HTML\n\ndef print_step(emoji, title, status=\"\"):\n    print(f\"\\n{emoji} {title}\")\n    print(\"=\"*60)\n    if status:\n        print(status)\n\ndef run_command(cmd, description, silent=True):\n    \"\"\"Run a command and handle output\"\"\"\n    print(f\"  {description}...\", end=\"\", flush=True)\n    try:\n        if silent:\n            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        else:\n            result = subprocess.run(cmd, shell=True, check=True)\n        print(\" âœ…\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\" âŒ\\n{e}\")\n        return False\n\nprint(\"\")\nprint(\"=\"*60)\nprint(\"     ğŸ¨ MASUKA V2 - Complete Setup Starting\")\nprint(\"=\"*60)\n\n# ============================================================\n# STEP 1: Check GPU\n# ============================================================\nprint_step(\"1ï¸âƒ£\", \"Checking GPU\")\nrun_command(\"nvidia-smi -L\", \"Detecting GPU\", silent=True)\n\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    print(f\"  GPU: {gpu_name}\")\n    print(f\"  VRAM: {vram:.1f} GB\")\n    print(\"  âœ… GPU Ready!\")\nelse:\n    print(\"  âŒ No GPU detected!\")\n    print(\"  Go to: Runtime â†’ Change runtime type â†’ GPU\")\n    sys.exit(1)\n\n# ============================================================\n# STEP 2: Clone Repository\n# ============================================================\nprint_step(\"2ï¸âƒ£\", \"Cloning MASUKA V2 Repository\")\nif not os.path.exists('/content/masuka-v2'):\n    run_command(\n        \"git clone https://github.com/SamuelD27/masuka.git /content/masuka-v2\",\n        \"Cloning from GitHub\",\n        silent=False\n    )\nelse:\n    print(\"  âœ… Repository exists\")\n\nos.chdir('/content/masuka-v2')\n\n# ============================================================\n# STEP 3: Install Dependencies\n# ============================================================\nprint_step(\"3ï¸âƒ£\", \"Installing Python Dependencies\")\nprint(\"  This may take 2-3 minutes...\")\n\n# Install core dependencies\ndeps = [\n    \"fastapi>=0.115.0\",\n    \"uvicorn[standard]>=0.31.1\",\n    \"pydantic>=2.11.0\",\n    \"pydantic-settings>=2.5.2\",\n    \"python-multipart>=0.0.18\",\n    \"sqlalchemy==2.0.25\",\n    \"psycopg2-binary\",\n    \"alembic\",\n    \"redis\",\n    \"celery\",\n    \"flower\",\n    \"boto3\",\n    \"python-dotenv\",\n    \"pyyaml\",\n    \"pyngrok\"\n]\n\nfor dep in deps:\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep], \n                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\nprint(\"  âœ… Core dependencies installed\")\n\n# Verify\nimport fastapi, celery, pydantic\nprint(f\"  FastAPI: {fastapi.__version__}\")\nprint(f\"  Celery: {celery.__version__}\")\nprint(f\"  Pydantic: {pydantic.__version__}\")\n\n# ============================================================\n# STEP 4: Install SimpleTuner\n# ============================================================\nprint_step(\"4ï¸âƒ£\", \"Installing SimpleTuner\")\nif not os.path.exists('/content/SimpleTuner'):\n    run_command(\n        \"git clone -q https://github.com/bghira/SimpleTuner /content/SimpleTuner\",\n        \"Cloning SimpleTuner\",\n        silent=False\n    )\n    \n    # Install SimpleTuner (uses setup.py)\n    print(\"  Installing SimpleTuner package...\", end=\"\", flush=True)\n    os.chdir('/content/SimpleTuner')\n    result = subprocess.run(\n        [sys.executable, \"setup.py\", \"install\"],\n        capture_output=True,\n        text=True\n    )\n    if result.returncode == 0:\n        print(\" âœ…\")\n    else:\n        # Try pip install\n        print(\" Using pip...\", end=\"\", flush=True)\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"],\n            stdout=subprocess.DEVNULL\n        )\n        print(\" âœ…\")\n    \n    os.chdir('/content/masuka-v2')\n    print(\"  âœ… SimpleTuner ready at /content/SimpleTuner\")\nelse:\n    print(\"  âœ… SimpleTuner already installed\")\n\n# ============================================================\n# STEP 5: Setup PostgreSQL\n# ============================================================\nprint_step(\"5ï¸âƒ£\", \"Setting up PostgreSQL\")\nrun_command(\"apt-get update -qq && apt-get install -y -qq postgresql postgresql-contrib\", \n            \"Installing PostgreSQL\")\nrun_command(\"service postgresql start\", \"Starting PostgreSQL\")\ntime.sleep(2)\n\n# Create database\ncommands = [\n    \"DROP DATABASE IF EXISTS masuka;\",\n    \"DROP USER IF EXISTS masuka;\",\n    \"CREATE USER masuka WITH PASSWORD 'password123';\",\n    \"CREATE DATABASE masuka OWNER masuka;\",\n    \"GRANT ALL PRIVILEGES ON DATABASE masuka TO masuka;\"\n]\nfor cmd in commands:\n    subprocess.run([\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", cmd], \n                   capture_output=True)\nprint(\"  âœ… Database 'masuka' created\")\n\n# ============================================================\n# STEP 6: Setup Redis\n# ============================================================\nprint_step(\"6ï¸âƒ£\", \"Setting up Redis\")\nrun_command(\"apt-get install -y -qq redis-server\", \"Installing Redis\")\nrun_command(\"redis-server --daemonize yes\", \"Starting Redis\")\ntime.sleep(1)\nrun_command(\"redis-cli ping\", \"Testing Redis\")\n\n# ============================================================\n# STEP 7: Configure Environment\n# ============================================================\nprint_step(\"7ï¸âƒ£\", \"Configuring Environment\")\n\nenv_content = \"\"\"# MASUKA V2 Colab Environment\nAPP_NAME=MASUKA V2\nDEBUG=true\n\n# Database\nDATABASE_URL=postgresql://masuka:password123@localhost:5432/masuka\n\n# Redis\nREDIS_URL=redis://localhost:6379/0\nCELERY_BROKER_URL=redis://localhost:6379/0\nCELERY_RESULT_BACKEND=redis://localhost:6379/0\n\n# JWT\nSECRET_KEY=colab-secret-key-change-in-production\nALGORITHM=HS256\nACCESS_TOKEN_EXPIRE_MINUTES=1440\n\n# Storage (AWS S3)\nS3_BUCKET=masuka-v2\nAWS_ACCESS_KEY_ID=AKIAWMUZGDEJYY6UYN5A\nAWS_SECRET_ACCESS_KEY=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\nS3_ENDPOINT_URL=https://s3.ap-southeast-1.amazonaws.com\nS3_REGION=ap-southeast-1\n\n# Paths\nSIMPLETUNER_PATH=/content/SimpleTuner\nMODELS_PATH=/content/models\nTEMP_PATH=/tmp/masuka\n\n# CORS\nCORS_ORIGINS=*\n\"\"\"\n\nos.makedirs('/content/masuka-v2/backend', exist_ok=True)\nwith open('/content/masuka-v2/backend/.env', 'w') as f:\n    f.write(env_content)\nprint(\"  âœ… Environment configured\")\n\n# ============================================================\n# STEP 8: Initialize Database\n# ============================================================\nprint_step(\"8ï¸âƒ£\", \"Initializing Database\")\nos.makedirs('/tmp/masuka/uploads', exist_ok=True)\nos.makedirs('/tmp/masuka/training', exist_ok=True)\nos.makedirs('/content/models', exist_ok=True)\nprint(\"  âœ… Directories created\")\n\n# Note: Database tables will be created when backend starts\n\n# ============================================================\n# STEP 9: Start FastAPI Backend\n# ============================================================\nprint_step(\"9ï¸âƒ£\", \"Starting FastAPI Backend\")\nos.chdir('/content/masuka-v2/backend')\n\n# Kill any existing uvicorn processes\nsubprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True)\ntime.sleep(2)\n\nbackend_process = subprocess.Popen(\n    ['uvicorn', 'app.main:app', '--host', '0.0.0.0', '--port', '8000', '--log-level', 'info'],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    text=True\n)\n\nprint(\"  â³ Waiting for backend to start...\")\ntime.sleep(15)\n\n# Test backend and verify routes\nimport requests\ntry:\n    response = requests.get('http://localhost:8000/health', timeout=5)\n    health = response.json()\n    print(f\"  âœ… Backend running: {health['app']} v{health['version']}\")\n    \n    # Verify datasets endpoint\n    response = requests.get('http://localhost:8000/api/datasets/', timeout=5)\n    if response.status_code == 200:\n        print(f\"  âœ… Datasets API loaded\")\n    else:\n        print(f\"  âš ï¸  Datasets API issue: {response.status_code}\")\n        \nexcept Exception as e:\n    print(f\"  âš ï¸  Backend may not be ready: {e}\")\n    print(f\"  Check backend logs if issues persist\")\n\n# ============================================================\n# STEP 10: Start Celery Worker\n# ============================================================\nprint_step(\"ğŸ”Ÿ\", \"Starting Celery Worker\")\n\n# Kill any existing celery processes\nsubprocess.run(['pkill', '-f', 'celery'], capture_output=True)\ntime.sleep(2)\n\ncelery_process = subprocess.Popen(\n    ['celery', '-A', 'app.tasks.celery_app', 'worker', \n     '--loglevel=info', '--concurrency=1', '-Q', 'training,generation'],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n    text=True\n)\n\nprint(\"  â³ Waiting for Celery to start...\")\ntime.sleep(5)\nprint(f\"  âœ… Celery worker running (GPU: {gpu_name})\")\n\n# ============================================================\n# STEP 11: Expose via ngrok\n# ============================================================\nprint_step(\"1ï¸âƒ£1ï¸âƒ£\", \"Creating Public URL\")\n\nfrom pyngrok import ngrok\n\n# Set ngrok auth token\nngrok.set_auth_token(\"33u4PSfJRAAdkBVl0lmMTo7LebK_815Q5PcJK6h68hM5PUAyM\")\n\n# Kill any existing tunnels\nngrok.kill()\ntime.sleep(2)\n\n# Create new tunnel\npublic_url = ngrok.connect(8000)\nprint(\"  âœ… Tunnel created\")\n\n# ============================================================\n# FINAL OUTPUT\n# ============================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ‰ MASUKA V2 Setup Complete!\")\nprint(\"=\"*70)\nprint(f\"\\nğŸ“¡ Your Public API URL:\")\nprint(f\"   {public_url}\")\nprint(f\"\\nğŸ“š Interactive Docs:\")\nprint(f\"   {public_url}/docs\")\nprint(f\"\\nğŸ” Health Check:\")\nprint(f\"   {public_url}/health\")\nprint(\"\\n\" + \"=\"*70)\nprint(\"\\nâœ… Services Running:\")\nprint(\"   â€¢ PostgreSQL Database\")\nprint(\"   â€¢ Redis Cache & Queue\")\nprint(\"   â€¢ FastAPI Backend\")\nprint(f\"   â€¢ Celery Worker (GPU: {gpu_name})\")\nprint(\"   â€¢ SimpleTuner Ready\")\nprint(\"\\nâš ï¸  Keep this notebook running during training!\")\nprint(\"\\nğŸ“– Use the helper cells below to:\")\nprint(\"   1. Upload training images & start training\")\nprint(\"   2. Monitor progress (automatic)\")\nprint(\"=\"*70)\n\n# Store for other cells (convert to string!)\nAPI_URL = str(public_url).replace('NgrokTunnel: \"', '').split('\"')[0]\nif not API_URL.startswith('http'):\n    API_URL = str(public_url)\nprint(f\"\\nâœ… API_URL stored: {API_URL}\")\n\nBACKEND_PROCESS = backend_process\nCELERY_PROCESS = celery_process"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage"
   },
   "source": [
    "---\n",
    "\n",
    "# ğŸ“– Usage - Upload & Train\n",
    "\n",
    "Run the cells below after setup completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_train",
    "cellView": "form"
   },
   "outputs": [],
   "source": "#@title ğŸ“¤ Upload Images, Start Training & Monitor Progress\n#@markdown Upload 15-25 images of your subject (JPG/PNG). Training will start and be monitored automatically.\n\nfrom google.colab import files\nimport requests\nimport io\nimport time\nfrom IPython.display import clear_output\n\n# Ensure API_URL is a string\nif not isinstance(API_URL, str):\n    API_URL = str(API_URL).replace('NgrokTunnel: \"', '').split('\"')[0]\nprint(f\"Using API: {API_URL}\")\n\n# Get parameters\ndataset_name = \"My LoRA Dataset\" #@param {type:\"string\"}\ntrigger_word = \"myface\" #@param {type:\"string\"}\nlearning_rate = 0.0001 #@param {type:\"number\"}\ntraining_steps = 2000 #@param {type:\"slider\", min:500, max:5000, step:100}\n\nprint(\"ğŸ“¤ Upload Your Training Images\")\nprint(\"=\"*60)\nprint(\"Click 'Choose Files' and select 15-25 images...\\n\")\n\n# Upload files\nuploaded = files.upload()\nprint(f\"\\nâœ… Received {len(uploaded)} files\")\n\n# Prepare files\nfile_list = []\nfor filename, data in uploaded.items():\n    if filename.lower().endswith(('.jpg', '.jpeg')):\n        content_type = 'image/jpeg'\n    elif filename.lower().endswith('.png'):\n        content_type = 'image/png'\n    elif filename.lower().endswith('.webp'):\n        content_type = 'image/webp'\n    else:\n        continue\n    file_list.append(('files', (filename, io.BytesIO(data), content_type)))\n\n# Create dataset\nprint(\"\\nğŸ“¦ Creating dataset...\")\nresponse = requests.post(\n    f\"{API_URL}/api/datasets/\",\n    json={\"name\": dataset_name, \"trigger_word\": trigger_word}\n)\n\n# Check for errors\nif response.status_code != 201:\n    print(f\"âŒ Error creating dataset:\")\n    print(f\"Status: {response.status_code}\")\n    print(f\"Response: {response.text}\")\n    raise Exception(f\"Failed to create dataset: {response.text}\")\n\ndataset = response.json()\ndataset_id = dataset['id']\nprint(f\"âœ… Dataset: {dataset_id}\")\n\n# Upload images\nprint(\"\\nğŸ“¤ Uploading to server...\")\nresponse = requests.post(\n    f\"{API_URL}/api/datasets/{dataset_id}/upload\",\n    files=file_list\n)\n\nif response.status_code != 200:\n    print(f\"âŒ Error uploading files:\")\n    print(f\"Status: {response.status_code}\")\n    print(f\"Response: {response.text}\")\n    raise Exception(f\"Failed to upload files: {response.text}\")\n\nresult = response.json()\nprint(f\"âœ… Uploaded {result['uploaded_count']} images\")\n\n# Start training\nprint(\"\\nğŸš€ Starting training...\")\nresponse = requests.post(\n    f\"{API_URL}/api/training/flux\",\n    json={\n        \"name\": f\"{dataset_name} - v1\",\n        \"model_type\": \"flux_image\",\n        \"dataset_id\": dataset_id,\n        \"learning_rate\": learning_rate,\n        \"steps\": training_steps,\n        \"network_dim\": 32,\n        \"network_alpha\": 16,\n        \"resolution\": 1024,\n        \"trigger_word\": trigger_word\n    }\n)\n\nif response.status_code != 200:\n    print(f\"âŒ Error starting training:\")\n    print(f\"Status: {response.status_code}\")\n    print(f\"Response: {response.text}\")\n    raise Exception(f\"Failed to start training: {response.text}\")\n\ntraining = response.json()\nSESSION_ID = training['session_id']\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ… Training Started!\")\nprint(\"=\"*60)\nprint(f\"Session ID: {SESSION_ID}\")\nprint(f\"Status: {training['status']}\")\nprint(f\"\\nâ±ï¸  Estimated Time: 45-60 minutes (T4) or 20-30 minutes (A100)\")\nprint(\"=\"*60)\n\n# Wait a moment for training to initialize\ntime.sleep(5)\n\n# ============================================================\n# NOW AUTOMATICALLY START MONITORING\n# ============================================================\nprint(\"\\n\\nğŸ“Š Monitoring Training Progress...\\n\")\n\nstart_time = time.time()\nlast_step = 0\n\nwhile True:\n    try:\n        response = requests.get(f\"{API_URL}/api/training/{SESSION_ID}\")\n        status = response.json()\n        \n        clear_output(wait=True)\n        \n        print(\"=\"*70)\n        print(f\"ğŸ¨ {status['name']}\")\n        print(\"=\"*70)\n        \n        current_status = status['status']\n        status_emoji = {\n            'pending': 'â³', 'training': 'ğŸ”„',\n            'completed': 'âœ…', 'failed': 'âŒ', 'cancelled': 'ğŸ›‘'\n        }\n        print(f\"\\nStatus: {status_emoji.get(current_status, 'â“')} {current_status.upper()}\")\n        \n        if status.get('current_step') and status.get('total_steps'):\n            current = status['current_step']\n            total = status['total_steps']\n            progress = (current / total) * 100\n            \n            print(f\"\\nProgress: {progress:.1f}% ({current:,}/{total:,} steps)\")\n            \n            # Progress bar\n            bar_length = 50\n            filled = int(bar_length * progress / 100)\n            bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)\n            print(f\"[{bar}] {progress:.1f}%\")\n            \n            if status.get('current_loss'):\n                print(f\"\\nLoss: {status['current_loss']:.6f}\")\n            \n            # Time estimate\n            if current > last_step and last_step > 0:\n                elapsed = time.time() - start_time\n                time_per_step = elapsed / current\n                eta_seconds = time_per_step * (total - current)\n                print(f\"\\nElapsed: {elapsed/60:.1f}m | ETA: {eta_seconds/60:.1f}m\")\n            \n            last_step = current\n        \n        print(\"\\n\" + \"=\"*70)\n        \n        if current_status in ['completed', 'failed', 'cancelled']:\n            if current_status == 'completed':\n                print(\"\\nğŸ‰ Training Completed!\")\n                print(\"âœ… Model uploaded to S3: s3://masuka-v2/models/\")\n                print(f\"\\nğŸ“¦ Session ID: {SESSION_ID}\")\n            elif current_status == 'failed':\n                print(f\"\\nâŒ Training Failed: {status.get('error_message', 'Unknown')}\")\n            else:\n                print(\"\\nğŸ›‘ Training Cancelled\")\n            break\n        \n        print(\"\\nâ³ Updating in 10 seconds... (Press Stop to exit monitoring)\")\n        time.sleep(10)\n        \n    except KeyboardInterrupt:\n        print(\"\\n\\nâš ï¸  Monitoring stopped. Training continues in background.\")\n        print(f\"ğŸ“‹ Session ID: {SESSION_ID}\")\n        print(f\"ğŸ’¡ Check status at: {API_URL}/api/training/{SESSION_ID}\")\n        break\n    except Exception as e:\n        print(f\"\\nâŒ Error: {e}\")\n        time.sleep(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fix_api_url_helper",
    "cellView": "form"
   },
   "outputs": [],
   "source": "#@title ğŸ”§ Debug Helper: Check Backend Status & Restart\n#@markdown Run this if you get 404 errors or other API issues\n\nimport requests\nimport subprocess\nimport time\n\n# Ensure API_URL is a string\nif 'API_URL' in globals():\n    if not isinstance(API_URL, str):\n        API_URL = str(API_URL).replace('NgrokTunnel: \"', '').split('\"')[0]\n    \n    print(f\"API URL: {API_URL}\\n\")\n    \n    # Test health endpoint\n    print(\"Testing /health endpoint...\")\n    try:\n        response = requests.get(f\"{API_URL}/health\", timeout=5)\n        if response.status_code == 200:\n            print(f\"âœ… Health check passed: {response.json()}\")\n        else:\n            print(f\"âŒ Health check failed: {response.status_code}\")\n    except Exception as e:\n        print(f\"âŒ Cannot reach backend: {e}\")\n    \n    # Test root endpoint\n    print(\"\\nTesting / endpoint...\")\n    try:\n        response = requests.get(f\"{API_URL}/\", timeout=5)\n        if response.status_code == 200:\n            print(f\"âœ… Root endpoint works: {response.json()}\")\n        else:\n            print(f\"âŒ Root failed: {response.status_code}\")\n    except Exception as e:\n        print(f\"âŒ Cannot reach root: {e}\")\n    \n    # Test datasets endpoint\n    print(\"\\nTesting /api/datasets endpoint...\")\n    try:\n        response = requests.get(f\"{API_URL}/api/datasets/\", timeout=5)\n        if response.status_code == 200:\n            print(f\"âœ… Datasets endpoint works!\")\n            datasets = response.json()\n            print(f\"   Found {len(datasets)} datasets\")\n        else:\n            print(f\"âŒ Datasets failed: {response.status_code}\")\n            print(f\"   Response: {response.text}\")\n    except Exception as e:\n        print(f\"âŒ Cannot reach datasets: {e}\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"ğŸ’¡ If endpoints are failing, the backend may need restart.\")\n    print(\"   Run the setup cell again to restart all services.\")\n    print(\"=\"*70)\n    \nelse:\n    print(\"âŒ API_URL not found. Run the setup cell first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Complete!\n",
    "\n",
    "**Your trained model is in:**\n",
    "- S3 Bucket: `masuka-v2`\n",
    "- Path: `models/{user_id}/{session_id}/flux_lora.safetensors`\n",
    "\n",
    "**Download from AWS S3 Console or:**\n",
    "```bash\n",
    "aws s3 cp s3://masuka-v2/models/.../flux_lora.safetensors ./\n",
    "```\n",
    "\n",
    "**Next:** Use your LoRA with ComfyUI, Auto1111, or any Flux-compatible tool!\n",
    "\n",
    "---\n",
    "\n",
    "Created by **MASUKA V2** | Phase 2 Complete âœ…"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}